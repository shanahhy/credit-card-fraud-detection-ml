{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/shanahhy/shanahhy/blob/main/Credit_Card_Fraud_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YqV30ydFdoW"
   },
   "source": [
    "Credit Card Fraud Detection Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I6xUzUi5Fumm",
    "outputId": "71b25343-7faa-47a3-a362-fbd909205c08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
      "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
      "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (0.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.5)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pandas scikit-learn matplotlib seaborn xgboost lightgbm tensorflow imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5I-Tmx_EDJhi"
   },
   "source": [
    "Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "FFl30P8oG2h2",
    "outputId": "99fc521a-b3fa-4f5d-b745-ee36c105ee85"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-2f17d23a-7ca6-4b3c-9d66-6485438dd8fc\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-2f17d23a-7ca6-4b3c-9d66-6485438dd8fc\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sh0xWhG2DMlm"
   },
   "source": [
    "Load the Data into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPmxouBT6U4D"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('creditcard_2023.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlauaC8gDQLg"
   },
   "source": [
    "Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p49NH0jfBL_A",
    "outputId": "b5e2fa8a-ff83-479d-d740-f4e0989cefb6"
   },
   "outputs": [],
   "source": [
    "print(df.head())\n",
    "print(df.describe())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKBhSoWnDZVe"
   },
   "source": [
    "Visualize Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "tnhmtEQOBZAO",
    "outputId": "30ac2041-3765-4e51-c0aa-c9476bac883c"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.countplot(x='Class', data=df)\n",
    "plt.title('Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kn810p2LDccG"
   },
   "source": [
    "Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-lAA8faBboo"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df['Amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkB_vMw-DebQ"
   },
   "source": [
    "Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2iLAXgYWDgvC"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frGWORweDjvd"
   },
   "source": [
    "Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uY9X8puOBe0-"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrP_DT2mEO2y"
   },
   "source": [
    "Train Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hzh3igECES29",
    "outputId": "31d75033-f18b-4780-991f-7f5d8336b0a9"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    \"Neural Network\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=300)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"{name} trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKOkc998E0oG"
   },
   "source": [
    "Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "idy5D7g5E6uF",
    "outputId": "89bd0457-47d6-4d33-bdc3-9973b561c13d"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"== {name} ==\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"AUC-ROC: {roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42rlvvYFFK4M"
   },
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oO2pP2s5FLYx",
    "outputId": "00b2c844-3e3f-4e5b-f29d-4353ba758d88"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d')\n",
    "    plt.title(f'Confusion Matrix for {name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6YoaI3Wd1yS"
   },
   "source": [
    "Credit Card Fraud Detection Using Machine Learning\n",
    "Introduction\n",
    "Credit card fraud is a significant issue globally, costing businesses and consumers billions annually. With the rise of online transactions, the risk and complexity of fraudulent activities have increased, making it imperative to develop robust detection systems. This project aims to create a machine learning model that effectively detects fraudulent transactions, thereby reducing financial losses and enhancing consumer trust.\n",
    "\n",
    "The project outlines the problem's context, the dataset and preprocessing steps, the machine learning models explored, the performance metrics used, and the real-world implications of the findings. Additionally, it discusses the challenges faced during the project and how these were overcome, demonstrating how machine learning can be a powerful tool in combating credit card fraud, offering a scalable and efficient solution that goes beyond traditional methods.\n",
    "\n",
    "Problem Context\n",
    "Credit card fraud is not just a financial problem; it's a security challenge affecting millions globally. As the volume of online transactions continues to grow, so does the sophistication of fraudsters, who continually evolve their tactics, making it difficult for traditional systems to keep up. Current solutions often rely on human-led investigations or simple rule-based systems that can miss novel fraud patterns. These systems are reactive rather than proactive, detecting fraud after it has occurred, which can lead to significant financial losses and damage to consumer trust.\n",
    "\n",
    "The objective of this project is to leverage machine learning to proactively detect fraud by learning the subtle characteristics of fraudulent transactions. Machine learning models can analyze vast amounts of data quickly, identifying patterns that might be invisible to human analysts. By framing this as a supervised learning task—distinguishing between fraudulent and legitimate transactions, we can systematically study the factors contributing to suspicious behavior. Our model's performance will be evaluated against existing benchmarks to ensure it offers a competitive edge in fraud detection. Ultimately, this project aims to provide a solution that is both academically informative and practically relevant for real-world financial applications.\n",
    "\n",
    "Dataset and Preprocessing\n",
    "For this project, the Kaggle Credit Card Fraud Detection Dataset was utilized, containing over 550,000 anonymized records of credit card transactions from European cardholders. The data includes 28 features that are principal components derived from a PCA transformation, along with 'Amount' and 'Time' features, ensuring privacy while providing a rich dataset for analysis.\n",
    "\n",
    "A primary challenge with this dataset is its severe class imbalance; fraudulent transactions account for only about 0.17% of all transactions. This imbalance can severely impact the learning process of machine learning models, as they may become biased towards the majority class. To address this, several preprocessing steps were performed. The 'Amount' feature was normalized to ensure it was on the same scale as the PCA components. SMOTE, a technique for generating synthetic samples of the minority class, was used to balance the dataset. This step was crucial in ensuring that the models could learn effectively from both classes. Additionally, the data was split into training, validation, and test sets to evaluate the models’ performance on unseen data. These preprocessing steps were vital for building robust and accurate models capable of handling real-world data.\n",
    "\n",
    "Machine Learning Models\n",
    "In exploring the best approach for detecting credit card fraud, a range of machine learning models was experimented with, each chosen for its unique strengths and potential to capture complex patterns associated with fraudulent transactions. The models included Logistic Regression, Decision Tree, Random Forest, Gradient Boosting (specifically XGBoost), and a Neural Network with a Multi-Layer Perceptron architecture.\n",
    "\n",
    "Logistic Regression was used as a baseline model due to its simplicity and interpretability, providing insights through model coefficients, valuable for understanding the impact of different features. Decision Trees offer an interpretable structure for identifying key transaction features, though they are prone to overfitting. Random Forests, ensembles of decision trees, help mitigate this issue by averaging out predictions, making them robust and strong performers on tabular data.\n",
    "\n",
    "Gradient Boosting models, such as XGBoost, are known for their high accuracy and ability to handle complex relationships by building ensembles in a sequential manner to correct errors from previous trees. Lastly, Neural Networks, specifically Multi-Layer Perceptrons, were included for their ability to model complex feature interactions. However, they require careful tuning to avoid overfitting, especially with limited data. By comparing these models, the most effective approach for fraud detection was identified, balancing accuracy with interpretability and computational efficiency.\n",
    "\n",
    "Performance Metrics and Evaluation\n",
    "Evaluating the performance of fraud detection models requires careful consideration of metrics, especially due to the class imbalance inherent in the dataset. Traditional metrics like accuracy can be misleading, as a model could achieve high accuracy simply by predicting the majority class. Therefore, metrics that provide a more nuanced view of model performance were focused on: precision, recall, F1-score, and AUC-ROC.\n",
    "\n",
    "Precision measures the proportion of correctly identified fraudulent transactions out of all transactions flagged as fraud, crucial for minimizing false positives, which can disrupt legitimate users. Recall measures the proportion of actual fraudulent transactions correctly identified, important for ensuring fraudulent activities are not missed. The F1-score, the harmonic mean of precision and recall, provides a balanced measure accounting for both false positives and false negatives.\n",
    "\n",
    "AUC-ROC, the area under the receiver operating characteristic curve, indicates the trade-off between the true positive rate and false positive rate. A higher AUC suggests stronger separability between fraud and legitimate classes. Additionally, confusion matrices were used to detail true/false positives and negatives, pinpointing where each model struggles. These metrics are well-suited for heavily imbalanced classification tasks and provide a comprehensive view of the models' capabilities in detecting fraud.\n",
    "\n",
    "Related Work and Approach\n",
    "Credit card fraud detection has been extensively researched, with many studies relying on traditional classifiers like Logistic Regression or Decision Trees. While these models often report high accuracy, they may not adequately address class imbalance issues, leading to potentially misleading results. This approach differs by focusing on ensemble and boosting methods, such as XGBoost, capable of capturing subtler patterns that single classifiers might miss.\n",
    "\n",
    "Class imbalance strategies were closely attended to, with techniques like SMOTE for oversampling and class-weight balancing evaluated to improve performance on rare fraud cases. These methods ensure the minority class, critical in fraud detection, is adequately represented during training. Hyperparameter tuning was also a key component, using techniques like Grid Search and Randomized Search to optimize parameters of all chosen models, maximizing performance.\n",
    "\n",
    "By integrating these strategies, the approach aims to build a robust fraud detection model achieving high accuracy while maintaining high precision and recall, ensuring effectiveness at identifying fraud and practicality for real-world applications, where minimizing false positives and negatives is crucial.\n",
    "\n",
    "Training Plan and Environment Setup\n",
    "To implement this project, a comprehensive training plan and environment were set up. The first step ensured all necessary tools and libraries were installed, including Python, Jupyter Notebook, and libraries like NumPy, Pandas, Scikit-learn, Matplotlib, Seaborn, XGBoost, LightGBM, TensorFlow/Keras, and Imbalanced-learn, providing the foundation for data manipulation, model building, and evaluation.\n",
    "\n",
    "Data preparation involved cleaning any missing data, scaling the 'Amount' feature, and applying techniques for class imbalance like SMOTE. The dataset was split into training, validation, and test sets, typically using a 70/15/15 split, allowing for effective model training, validation, and unbiased evaluation on unseen data.\n",
    "\n",
    "Model training involved using cross-validation to ensure consistent performance across different data subsets. Hyperparameter tuning methods like Grid Search and Randomized Search were experimented with to optimize model parameters. Evaluation and model selection were based on metrics like AUC-ROC, F1-score, and recall for the fraud class, with confusion matrices providing additional insights into model performance. Finally, the iteration and refinement phase involved error analysis to identify misclassification patterns and consider additional feature engineering if necessary. This structured approach ensured a thorough exploration of models and techniques, leading to the development of an effective fraud detection system.\n",
    "\n",
    "Results and Findings\n",
    "After implementing the various models and evaluating their performance, Gradient Boosting using XGBoost consistently achieved the best results, demonstrating a high AUC-ROC score, indicating strong separability between fraudulent and legitimate transactions. This model maintained a good balance between precision and recall, minimizing false positives and negatives, critical in real-world applications where both types of errors have significant consequences.\n",
    "\n",
    "The Random Forest model also performed well, benefiting from its ensemble approach, mitigating overfitting and capturing complex patterns effectively. Logistic Regression, while less complex, provided valuable insights due to its interpretability, helping understand the impact of different features on fraud detection.\n",
    "\n",
    "A key finding was the importance of addressing class imbalance. Models trained without balancing techniques like SMOTE struggled to identify fraudulent transactions, highlighting the need for careful preprocessing. Hyperparameter tuning further enhanced model performance, underscoring the value of optimizing model parameters rather than relying on defaults.\n",
    "\n",
    "These results demonstrate the potential of machine learning in enhancing fraud detection systems. By leveraging advanced algorithms and addressing class imbalance challenges, the developed models offer a scalable and efficient solution that can be integrated into existing systems to improve fraud detection rates while minimizing disruptions to legitimate users.\n",
    "\n",
    "Challenges and Solutions\n",
    "Throughout this project, several challenges were encountered, each requiring careful consideration and strategic solutions. The most significant challenge was the severe class imbalance in the dataset, with fraudulent transactions comprising a tiny fraction of the total. This imbalance often leads models to be biased towards the majority class, resulting in poor detection of fraud cases. To overcome this, SMOTE was implemented, generating synthetic samples of the minority class, effectively balancing the dataset and allowing models to learn more effectively from both classes.\n",
    "\n",
    "Another challenge was the risk of overfitting, particularly with complex models like Neural Networks. Overfitting occurs when a model learns the training data too well, capturing noise rather than underlying patterns, resulting in poor generalization to new data. To mitigate this, cross-validation was used, ensuring consistent performance across different data subsets, and regularization methods were applied to penalize overly complex models.\n",
    "\n",
    "These challenges and their solutions highlight the complexity of fraud detection and the need for a multifaceted approach. By addressing these issues, the project not only achieved its objectives but also contributed valuable insights into the application of machine learning for fraud detection.\n",
    "\n",
    "Real-World Applications and Conclusion\n",
    "The implications of this project extend beyond academic interest to practical applications in the financial sector. Financial institutions can leverage the developed models to enhance their fraud detection systems, reducing financial losses and improving customer trust. These models can be integrated into existing systems to monitor transactions in real time, providing an additional layer of security that is both efficient and scalable.\n",
    "\n",
    "E-commerce platforms stand to benefit as well, implementing these models to prevent fraudulent activities before they occur, safeguarding both the business and its customers. The use of machine learning in fraud detection can also inspire further innovations in cybersecurity and risk management, making online transactions safer for everyone involved.\n",
    "\n",
    "In conclusion, this project successfully demonstrated the potential of machine learning for credit card fraud detection. By addressing class imbalance and leveraging advanced models like XGBoost, high detection rates were achieved while minimizing disruptions to legitimate users. The findings advance academic understanding of fraud detection and offer practical solutions applicable in real-world settings. Future work could include incorporating additional features, exploring other datasets, or deploying the model in a real-time environment.\n",
    "\n",
    "References  - Dal Pozzolo, A., Boracchi, G., Caelen, O., Alippi, C., & Bontempi, G. (2017). Credit Card Fraud\n",
    "Detection and Concept-Drift Adaptation with Delayed Supervised Information. 2017 International\n",
    "Joint Conference on Neural Networks (IJCNN). - Brown, I., & Mues, C. (2012). An experimental comparison of classification algorithms for\n",
    "imbalanced credit scoring data sets. Expert Systems with Applications, 39(3), 3446–3453. - Nelgiriyewithana, D. (2023). Credit card fraud detection dataset 2023 [Data set]. Kaggle. Retrieved\n",
    "from https://www.kaggle.com/datasets/nelgiriyewithana/credit-card-fraud-detection-dataset-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6H2eWxdcgI7V"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPUY3vox20HLPoTJ9wzjGrQ",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
